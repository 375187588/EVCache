{
    "docs": [
        {
            "location": "/", 
            "text": "What is EVCache?\n\n\nE\nphemeral \nV\nolatile mem\nCache\n is a fast, distributed, resilient, highly available data store for the cloud\n\n\n\n\nEphemeral\n : It is intended to store transient data that can disappear\n\n\nVolatile\n : In-Memory key-value store where data (strings, objects) can change\n\n\nCache\n : Uses \nmemcached\n \n in some cases backed by \nRocksDB", 
            "title": "Home"
        }, 
        {
            "location": "/#what-is-evcache", 
            "text": "E phemeral  V olatile mem Cache  is a fast, distributed, resilient, highly available data store for the cloud   Ephemeral  : It is intended to store transient data that can disappear  Volatile  : In-Memory key-value store where data (strings, objects) can change  Cache  : Uses  memcached    in some cases backed by  RocksDB", 
            "title": "What is EVCache?"
        }, 
        {
            "location": "/introduction/", 
            "text": "EVC\nache is a distributed Data Management service that stores data in-memory. EVCache vends a client library which is used by various applications (Web services, micro service, standalone apps, SPARK \n FLINK apps, etc) and it internally uses spymemcached to talk to cluster of memcached servers. EVCache clusters (a.k.a ServerGroups) are provisioned in each zone. EVCache writes each key-value to all zones (I.e. three writes for each application level write). Reads should primarily/only happen from the local zone to meet the SLA. Write-failures or WAW hazards are accepted as cases that lead to data inconsistencies across zones. That is acceptable because:\n\n\n\n\nIt is rare\n\n\nMost items have a short TTL (minutes) so that the duration for inconsistencies are constrained\n\n\nIf a particular cache needs high consistency then write failures can be configured to written to a kafka topic and EVCache consistency checker can fix such inconsistencies\n\n\nEVCache client can notify application of such failures and the application can chose to retry such failures\n\n\n\n\nRead failures are retried on other copies thus masking any transient failures. \n\n\nWorkload characteristics\n\n\n\n\nTypically a Read heavy load.\n\n\nAt Peak over 125K ops/sec for data size of 1KB on an m4.xlarge (1 Gbps)\n\n\nAround 12GB RAM per instance (when using m4.xlarge)\n\n\nKey sizes \n 255 bytes (Includes cache prifix).\n\n\nKey cannot have spaces or new line characters in them.\n\n\nValue sizes vary heavily and change over time. Value size larger than 1MB will be chunked on the server. \n\n\nLarger values take longer to retrieve\n\n\n\n\n\n\nTypical TTL few mins to few hours (some have multiple days, but we don't recommend long TTL)\n\n\nA cache hit rates in the 90% range.\n\n\nmemcached is configured to use 90% of RAM on an EC2 instance, and not provided any swap\n\n\nFor a value size of 1KB within a zone latency is \n 1ms, across zones \n 2ms\n\n\nTarget SLA: Avg Latency \n1ms; 99% \n 5ms\n\n\n99% is heavily influenced by JVM Pauses\n\n\n\n\n\n\n\n\nDefaults\n\n\n\n\nRead timeout set to 20 milliseconds, Goal is to answer fast, if not possible then fail fast and retry.\n\n\nRead queue size is 10. this will ensure we fail fast if a client tries to overload a server i.e. DOS attack\n\n\nWrite timeout is 2500 milliseconds. \n\n\nWrite queue size is 16K. i.e. we will try really hard to ensure we don't drop writes", 
            "title": "Introduction"
        }, 
        {
            "location": "/introduction/#workload-characteristics", 
            "text": "Typically a Read heavy load.  At Peak over 125K ops/sec for data size of 1KB on an m4.xlarge (1 Gbps)  Around 12GB RAM per instance (when using m4.xlarge)  Key sizes   255 bytes (Includes cache prifix).  Key cannot have spaces or new line characters in them.  Value sizes vary heavily and change over time. Value size larger than 1MB will be chunked on the server.   Larger values take longer to retrieve    Typical TTL few mins to few hours (some have multiple days, but we don't recommend long TTL)  A cache hit rates in the 90% range.  memcached is configured to use 90% of RAM on an EC2 instance, and not provided any swap  For a value size of 1KB within a zone latency is   1ms, across zones   2ms  Target SLA: Avg Latency  1ms; 99%   5ms  99% is heavily influenced by JVM Pauses     Defaults   Read timeout set to 20 milliseconds, Goal is to answer fast, if not possible then fail fast and retry.  Read queue size is 10. this will ensure we fail fast if a client tries to overload a server i.e. DOS attack  Write timeout is 2500 milliseconds.   Write queue size is 16K. i.e. we will try really hard to ensure we don't drop writes", 
            "title": "Workload characteristics"
        }, 
        {
            "location": "/features/", 
            "text": "Features\n\n\n\n\nA distributed, replicated cache with a simple memcached-semantics interface (get, set, touch, delete, etc.)\n\n\nLinear scalability of overall data size or network capacity (vs getting a bigger box)\n\n\nAny number of copies of data are supported (some clusters run with 2, others with 9)\n\n\nResiliency to failure. Individual instances can die, and do so regularly, without affecting client applications (many of the services at Netflix that use EVCache do so as a solitary storage mechanism, meaning the data has no other place it resides besides in EVCache.)\n\n\nOperations have topological awareness, retries, fallbacks, and other mechanisms to ensure successful completion (Optimization for AWS architecture)\n\n\nThe data in each key can be of any size (with data chunking)\n\n\nTransparent nearline global data replication \n\n\nSeamless cache deployments with no data loss\n\n\nDetailed Insights into operation and performance\n\n\nIn short: memcached is a single process that works very well on a single server. EVCache takes this and uses it as a fundamental building block.\n\n\nData is replicated across all regions using a custom Data Replication Service\n\n\nWhen any maintenance needs to be perfomed like\n\n\ndeploying new AMI\n\n\nincreasing or decreasing the cluster size \n\ndata from the old clusters are copied to new clusters\n\n\n\n\n\n\n\n\nKey canonicalization\n\n\n\n\nA key is \"canonicalized\" by prepending the cache prefix and the \":\" character. For example, if the prefix is \"cid\", the key is \"foo\" the key will be canonicalized to cid:foo. This eliminates namespace collisions when multiple caches share a single memcached instance.\n\n\n\n\nConsistency\n\n\n\n\nThis service makes no consistency guarantees whatsoever. Specifically:\n\n\nIt is purely best-effort\n\n\nThere is no guarantee that the contents of two caches will converge\n\n\n\n\nDiscovery\n\n\n\n\nThe process of finding suitable caches is mediated by Discovery. The EVCache servers advertise themselves as EVCache App and register with discovery service. \n\n\nThe clients looking for a EVCache instances belonging to a particular app get the list of instances from Discovery and the EVCache client manages them.\n\n\n\n\nMirroring/Replication\n\n\n\n\nData is always mirrored (replicated) between zones. That is, two cache servers in differing zones supporting the same cache will contain the same data.\n\n\nWithin a zone, data will be sharded across the set of instances. The sharding of the data is based on Ketama consisten hashing algorithm.\n\n\n\n\nRead Retries\n\n\n\n\nIn general, caches in the local zone are preferred while reading.\n\n\nIf no caches are available within your zone, a cache from another zone is chosen at random.\n\n\nNote that if you are sharding, adding a cache instance is a disruptive process, because it changes the results of the consistent hashing algorithm. Some fraction of the cache's entries will become unreachable, and they will languish in obscurity until their object lifetimes are reached. Storing the item anew will result it in being assigned to the correct node, and the cache will then behave as expected.\n\n\n\n\n\n\n\n\nConnection Pools and Management\n\n\n\n\nCreate a Pool of connections to each EVCache App at startup\n\n\nSeparate pools for Read and Write Operations\n\n\n\n\nResilient to state in Discovery\n\n\n\n\nBe passive when disconnecting from EVCache server i.e. if discovery drops a server the client should not remove it until the connection is lost. \n\n\nVerification of instance replacement before rehashing. This ensures key movement is minimized when disruptions happen.\n\n\nServerGroup config can be provided by System properties or FastProperties and can be used to bootstrap. Ideal for situations where the client is in a non Discovery/Eureka environment.\n\n\nUses spinnaker API as a backup to find EVCache instances\n\n\n\n\nWrite only Clusters\n\n\n\n\nSet a Server Group/zone to write only mode\n\n\nWe can employ this when we are changing cluster sizes or pushing new AMI\n\n\n\n\nLatch based API\n\n\n\n\nStatus about write operations can be obtained using Latch.\n\n\nCan be used to block for a specified duration until the latch policy is met.\n\n\n\n\nGC Pauses\n\n\n\n\nRead operation caught in-flight can linger after pause before failing or retrying\n\n\n\n\nDynamic compression\n\n\n\n\nEVCache Transcoder can be configured to perform data compression dynamically\n\n\nDecompression will be done only if the compress flag is set\n\n\n\n\nClient side caching\n\n\n\n\nLRU based in-memory caching inside of EVCache client.\n\n\nCan be enabled dynamically and transparent to the app. \n\n\nIdeal for immutable objects with duplicate reads in short duration.\n\n\n\n\nMBeans \n JMX Operations\n\n\n\n\nConnection Stauts\n\n\nQueue length for Reads, Writes and input\n\n\nCache Hits/Misses for read Operations\n\n\nErrors and Timeouts\n\n\nActive \n Inactive Instance\n\n\nRefresh connection pools \n\n\nClear the Operation Queue\n\n\n\n\nTypical EVCache Cluster Example", 
            "title": "Features"
        }, 
        {
            "location": "/features/#features", 
            "text": "A distributed, replicated cache with a simple memcached-semantics interface (get, set, touch, delete, etc.)  Linear scalability of overall data size or network capacity (vs getting a bigger box)  Any number of copies of data are supported (some clusters run with 2, others with 9)  Resiliency to failure. Individual instances can die, and do so regularly, without affecting client applications (many of the services at Netflix that use EVCache do so as a solitary storage mechanism, meaning the data has no other place it resides besides in EVCache.)  Operations have topological awareness, retries, fallbacks, and other mechanisms to ensure successful completion (Optimization for AWS architecture)  The data in each key can be of any size (with data chunking)  Transparent nearline global data replication   Seamless cache deployments with no data loss  Detailed Insights into operation and performance  In short: memcached is a single process that works very well on a single server. EVCache takes this and uses it as a fundamental building block.  Data is replicated across all regions using a custom Data Replication Service  When any maintenance needs to be perfomed like  deploying new AMI  increasing or decreasing the cluster size  \ndata from the old clusters are copied to new clusters", 
            "title": "Features"
        }, 
        {
            "location": "/features/#key-canonicalization", 
            "text": "A key is \"canonicalized\" by prepending the cache prefix and the \":\" character. For example, if the prefix is \"cid\", the key is \"foo\" the key will be canonicalized to cid:foo. This eliminates namespace collisions when multiple caches share a single memcached instance.", 
            "title": "Key canonicalization"
        }, 
        {
            "location": "/features/#consistency", 
            "text": "This service makes no consistency guarantees whatsoever. Specifically:  It is purely best-effort  There is no guarantee that the contents of two caches will converge", 
            "title": "Consistency"
        }, 
        {
            "location": "/features/#discovery", 
            "text": "The process of finding suitable caches is mediated by Discovery. The EVCache servers advertise themselves as EVCache App and register with discovery service.   The clients looking for a EVCache instances belonging to a particular app get the list of instances from Discovery and the EVCache client manages them.", 
            "title": "Discovery"
        }, 
        {
            "location": "/features/#mirroringreplication", 
            "text": "Data is always mirrored (replicated) between zones. That is, two cache servers in differing zones supporting the same cache will contain the same data.  Within a zone, data will be sharded across the set of instances. The sharding of the data is based on Ketama consisten hashing algorithm.", 
            "title": "Mirroring/Replication"
        }, 
        {
            "location": "/features/#read-retries", 
            "text": "In general, caches in the local zone are preferred while reading.  If no caches are available within your zone, a cache from another zone is chosen at random.  Note that if you are sharding, adding a cache instance is a disruptive process, because it changes the results of the consistent hashing algorithm. Some fraction of the cache's entries will become unreachable, and they will languish in obscurity until their object lifetimes are reached. Storing the item anew will result it in being assigned to the correct node, and the cache will then behave as expected.", 
            "title": "Read Retries"
        }, 
        {
            "location": "/features/#connection-pools-and-management", 
            "text": "Create a Pool of connections to each EVCache App at startup  Separate pools for Read and Write Operations", 
            "title": "Connection Pools and Management"
        }, 
        {
            "location": "/features/#resilient-to-state-in-discovery", 
            "text": "Be passive when disconnecting from EVCache server i.e. if discovery drops a server the client should not remove it until the connection is lost.   Verification of instance replacement before rehashing. This ensures key movement is minimized when disruptions happen.  ServerGroup config can be provided by System properties or FastProperties and can be used to bootstrap. Ideal for situations where the client is in a non Discovery/Eureka environment.  Uses spinnaker API as a backup to find EVCache instances", 
            "title": "Resilient to state in Discovery"
        }, 
        {
            "location": "/features/#write-only-clusters", 
            "text": "Set a Server Group/zone to write only mode  We can employ this when we are changing cluster sizes or pushing new AMI", 
            "title": "Write only Clusters"
        }, 
        {
            "location": "/features/#latch-based-api", 
            "text": "Status about write operations can be obtained using Latch.  Can be used to block for a specified duration until the latch policy is met.", 
            "title": "Latch based API"
        }, 
        {
            "location": "/features/#gc-pauses", 
            "text": "Read operation caught in-flight can linger after pause before failing or retrying", 
            "title": "GC Pauses"
        }, 
        {
            "location": "/features/#dynamic-compression", 
            "text": "EVCache Transcoder can be configured to perform data compression dynamically  Decompression will be done only if the compress flag is set", 
            "title": "Dynamic compression"
        }, 
        {
            "location": "/features/#client-side-caching", 
            "text": "LRU based in-memory caching inside of EVCache client.  Can be enabled dynamically and transparent to the app.   Ideal for immutable objects with duplicate reads in short duration.", 
            "title": "Client side caching"
        }, 
        {
            "location": "/features/#mbeans-jmx-operations", 
            "text": "Connection Stauts  Queue length for Reads, Writes and input  Cache Hits/Misses for read Operations  Errors and Timeouts  Active   Inactive Instance  Refresh connection pools   Clear the Operation Queue", 
            "title": "MBeans &amp; JMX Operations"
        }, 
        {
            "location": "/features/#typical-evcache-cluster-example", 
            "text": "", 
            "title": "Typical EVCache Cluster Example"
        }, 
        {
            "location": "/using/", 
            "text": "Initialize\n\n\nBelow describes the best practices using EVCache client.\n\n\nGovernated Server\n\n\nYou can do this in your StartServer.java (or the class that extends BaseServerLifecycleListener).\n\n\npublic class StartServer extends BaseServerLifecycleListener {\n    protected void initialize(ServletContextEvent event) throws Exception {\n        Injector injector = getInjector();\n        injector.getInstance(com.netflix.evcache.EVCacheClientModule.class);\n    }\n}\n\n\n\n\nClient Library\n\n\nWhile initializing EVCacheClient Library we recommend you can create connections to EVCache servers your library needs.\nYou can achieve this by doing the following\n\n\npublic class MyClientLibrary {\n    @Inject\n    public MyClientLibrary(EVCache.Builder evCacheBuilder, \nOther Librariess your client needs\n) {\n        ....\n        EVCache evCache = evCacheBuilder\n            .setAppName(\nEVCACHE App Name in upper case\n)\n            .setCachePrefix(\ncid\n) //Set the optional cache prefix\n            .enableRetry() //Enable Retry\n            .setDefaultTTL(3600) //Set TTL for 1 Hour\n            .setTranscoder(...) //The transcoder to serialize and deserialize data stored in evcache\n            .enableExceptionPropagation() //If exceptions need to be propagated else null is returned in case of exception\n            .build();\n        ....\n    }\n}\n\n\n\n\nClient Module\n\n\nIf you have a client moudle you can initialize EVCache as below\n\n\npublic class MyClientModule extends AbstractModule {\n\n    @Override\n    protected void configure() {\n        install(new EVCacheClientModule());\n        //install other modules\n    }\n\n}\n\n\n\n\nTo obtain EVCache instance the first step is to import the necessary packages and create the cache. Specify the EVCache App Name, cache prefix (all keys will be prepended by this name so choose wisely) and the Default TTL and build the instance. Optionally you can also specify the transcoder and if your instance needs to do zone fallback.\n\n\npublic class MyClass {\n    @Inject\n    public MyClass(EVCache.Builder evCacheBuilder, \nOther Librariess your client needs\n) {\n            ....\n            EVCache evCache = evCacheBuilder\n                .setAppName(\nEVCACHE App Name in upper case\n)\n                .setCachePrefix(\ncid\n) //Set the optional cache prefix\n                .enableRetry() //Enable Retry\n                .setDefaultTTL(3600) //Set TTL for 1 Hour\n                .setTranscoder(...) //The transcoder to serialize and deserialize data stored in evcache\n                .enableExceptionPropagation() //If exceptions need to be propagated else null is returned in case of exception\n                .build();\n            .....\n    }\n}\n\n\n\n\nAdding Data to EVCache\n\n\nTo set data you need to pass the key and value. You can also use the overloaded method and pass your custom transcoder and TTL(time to live) for you the data you are setting.\n\n\nNote: key cannot contain space or new line character\n\n\nValues are limited to 20MB maximum (minus serialization overhead)\n\n\nEVCacheLatch latch = client.set(\nkey\n, \nvalue\n, Policy.ALL_MINUS_1)\n\n\n\n\nAfter the latch is returned, you can wait on it for however long your SLA allows to ensure the write policy was met. Else the operation is unblocked but the write will proceed in the background. \nWaiting shorter than the write timeout will allow your code to continue but you will not know if the write was successful or not. You can inspect the EVCache write metrics to get status of such operations. \n\n\nboolean status = latch.await(20, TimeUnit.MILLISECONDS) \n// wait for 20 ms or until the policy is fulfilled, whichever is comes first\n//status boolean will be true if the operation was successful\n\n\n\n\nThere are several options for the policy, depending on your use case:\n\n\npublic static enum Policy {\n    ONE, QUORUM, ALL_MINUS_1, ALL\n}\n\n\n\n\nIt is generally not recommended to wait for all copies to confirm a write. If you do use the ALL policy, then it is possible for a bad box to back up your application servers because all of the latches will wait until the operation timeout or error.\n\n\nAnother approach is to use the non latch based set() method that return Future\n[]\n\n\nFuture\nBoolean\n[] status = myCache.set(\nkey\n,\nvalue\n);\n\n\n\n\nYou can inspect the status futures to ensure if that the data you have set is successful or not. Doing so will block the operation until we can determine if the operation was successful or not.\n\n\nfor(Future\nBoolean\n operationStatus :  status) {\n\n    /*\n        wait until the operation is completed. Note this blocks until the operation is completed. \n    */\n    operationStatus.get() \n\n    //To wait up to a specified duration you can pass the max block duration\n    operationStatus.get(50, TimeUnit.MILLISECONDS) \n    /*\n        the future is blocked for utmost 50 milliseconds. \n        If the operation is not completed by then the operation is cancelled the the thread is unblocked. \n    */\n}\n\n\n\n\nRetrieving data from EVCache\n\n\nTo retrieve the value associated with a key:\n\n\nString value = myCache.\nString\nget(\nkey\n);\n\n\n\n\nDelete data from EVCache:\n\n\nTo delete the data from EVCache\n\n\nFuture\nBoolean\n[] status = myCache.delete(\nkey\n);\n\n\n\n\nSimilar to set operation you can inspect the status flag to determine if the operation was successful or not. \n\n\nWriting Test Cases\n\n\nTo Init EVCache when AutoBinding is on\n\n\n//Make sure to turn on autobinding by setting the below property\n//platform.NFLifecycleUnitTester.ignoringAllAutoBindClasses=false\n\n//Init NFLifecycleUnitTester \nfinal NFLifecycleUnitTester unitTester = new NFLifecycleUnitTester(props);\n\n//Get instance of EVCache.Builder\nfinal EVCache.Builder evCacheBuilder = unitTester.getInjector().getInstance(EVCache.Builder.class);\n\n//Get instance of EVCache. Now you can perform all operations on it\nfinal EVCache evCache = evCacheBuilder.setAppName(\nEVCACHE_CINEPS\n).setCacheName(\ncid\n).enableZoneFallback().build();\n\nTo Init EVCache when AutoBinding is off\n//Make sure to turn off autobinding by setting the below property\n//platform.NFLifecycleUnitTester.ignoringAllAutoBindClasses=true\n\n//Init NFLifecycleUnitTester \nfinal NFLifecycleUnitTester unitTester = new NFLifecycleUnitTester(props, new EVCacheClientModule());\n\n//Get instance of EVCache.Builder\nfinal EVCache.Builder evCacheBuilder = unitTester.getInjector().getInstance(EVCache.Builder.class);\n\n//Get instance of EVCache. Now you can perform all operations on it\nfinal EVCache evCache = evCacheBuilder.setAppName(\nEVCACHE_CINEPS\n).setCacheName(\ncid\n).enableZoneFallback().build();\n\n\n\n\nIf you are seeing the following exception. \n\n\n No implementation for com.netflix.evcache.connection.IConnectionFactoryProvider was bound.\n  while locating com.google.inject.Provider\ncom.netflix.evcache.connection.IConnectionFactoryProvider\n\n    for parameter 2 at com.netflix.evcache.pool.EVCacheClientPoolManager.\ninit\n(EVCacheClientPoolManager.java:86)\n  while locating com.netflix.evcache.pool.EVCacheClientPoolManager\n    for field at com.netflix.evcache.EVCache$Builder._poolManager(EVCache.java:952)\n  while locating com.netflix.evcache.EVCache$Builder\n    for parameter 0 at com.netflix.ab.client.cache.common.ABEVCache.\ninit\n(ABEVCache.java:47)\n  while locating com.netflix.ab.client.cache.common.ABEVCache\n    for parameter 0 at com.netflix.ab.client.ABServiceClientManager.\ninit\n(ABServiceClientManager.java:41)\n  while locating com.netflix.ab.client.ABServiceClientManager\n    for parameter 0 at com.netflix.membership.client.MembershipClientLibrary.setAbServiceClientManager(MembershipClientLibrary.java:82)\n  at com.netflix.membership.client.MembershipClientLibrary.setAbServiceClientManager(MembershipClientLibrary.java:82)\n  while locating com.netflix.membership.client.MembershipClientLibrary\n    for parameter 0 at com.netflix.msl.userauth.SwitchProfileAuthenticationFactory.\ninit\n(SwitchProfileAuthenticationFactory.java:49)\n\n\n\n\nYou can also try to create LifecycleInjectorBuilder add EVCacheClientModule module to it. This should init all the necessary modules. \n\n\nLifecycleInjectorBuilder builder = LifecycleInjector.builder();\nbuilder.withModules( new EVCacheClientModule());\n\n\n\n\nYou might need to set the following properties if you are not able to talk to eureka \n\n\n-D@environment=test\n-D@region=us-west-2", 
            "title": "Using EVCache Clent"
        }, 
        {
            "location": "/using/#initialize", 
            "text": "Below describes the best practices using EVCache client.", 
            "title": "Initialize"
        }, 
        {
            "location": "/using/#governated-server", 
            "text": "You can do this in your StartServer.java (or the class that extends BaseServerLifecycleListener).  public class StartServer extends BaseServerLifecycleListener {\n    protected void initialize(ServletContextEvent event) throws Exception {\n        Injector injector = getInjector();\n        injector.getInstance(com.netflix.evcache.EVCacheClientModule.class);\n    }\n}", 
            "title": "Governated Server"
        }, 
        {
            "location": "/using/#client-library", 
            "text": "While initializing EVCacheClient Library we recommend you can create connections to EVCache servers your library needs.\nYou can achieve this by doing the following  public class MyClientLibrary {\n    @Inject\n    public MyClientLibrary(EVCache.Builder evCacheBuilder,  Other Librariess your client needs ) {\n        ....\n        EVCache evCache = evCacheBuilder\n            .setAppName( EVCACHE App Name in upper case )\n            .setCachePrefix( cid ) //Set the optional cache prefix\n            .enableRetry() //Enable Retry\n            .setDefaultTTL(3600) //Set TTL for 1 Hour\n            .setTranscoder(...) //The transcoder to serialize and deserialize data stored in evcache\n            .enableExceptionPropagation() //If exceptions need to be propagated else null is returned in case of exception\n            .build();\n        ....\n    }\n}", 
            "title": "Client Library"
        }, 
        {
            "location": "/using/#client-module", 
            "text": "If you have a client moudle you can initialize EVCache as below  public class MyClientModule extends AbstractModule {\n\n    @Override\n    protected void configure() {\n        install(new EVCacheClientModule());\n        //install other modules\n    }\n\n}  To obtain EVCache instance the first step is to import the necessary packages and create the cache. Specify the EVCache App Name, cache prefix (all keys will be prepended by this name so choose wisely) and the Default TTL and build the instance. Optionally you can also specify the transcoder and if your instance needs to do zone fallback.  public class MyClass {\n    @Inject\n    public MyClass(EVCache.Builder evCacheBuilder,  Other Librariess your client needs ) {\n            ....\n            EVCache evCache = evCacheBuilder\n                .setAppName( EVCACHE App Name in upper case )\n                .setCachePrefix( cid ) //Set the optional cache prefix\n                .enableRetry() //Enable Retry\n                .setDefaultTTL(3600) //Set TTL for 1 Hour\n                .setTranscoder(...) //The transcoder to serialize and deserialize data stored in evcache\n                .enableExceptionPropagation() //If exceptions need to be propagated else null is returned in case of exception\n                .build();\n            .....\n    }\n}", 
            "title": "Client Module"
        }, 
        {
            "location": "/using/#adding-data-to-evcache", 
            "text": "To set data you need to pass the key and value. You can also use the overloaded method and pass your custom transcoder and TTL(time to live) for you the data you are setting.  Note: key cannot contain space or new line character  Values are limited to 20MB maximum (minus serialization overhead)  EVCacheLatch latch = client.set( key ,  value , Policy.ALL_MINUS_1)  After the latch is returned, you can wait on it for however long your SLA allows to ensure the write policy was met. Else the operation is unblocked but the write will proceed in the background. \nWaiting shorter than the write timeout will allow your code to continue but you will not know if the write was successful or not. You can inspect the EVCache write metrics to get status of such operations.   boolean status = latch.await(20, TimeUnit.MILLISECONDS) \n// wait for 20 ms or until the policy is fulfilled, whichever is comes first\n//status boolean will be true if the operation was successful  There are several options for the policy, depending on your use case:  public static enum Policy {\n    ONE, QUORUM, ALL_MINUS_1, ALL\n}  It is generally not recommended to wait for all copies to confirm a write. If you do use the ALL policy, then it is possible for a bad box to back up your application servers because all of the latches will wait until the operation timeout or error.  Another approach is to use the non latch based set() method that return Future []  Future Boolean [] status = myCache.set( key , value );  You can inspect the status futures to ensure if that the data you have set is successful or not. Doing so will block the operation until we can determine if the operation was successful or not.  for(Future Boolean  operationStatus :  status) {\n\n    /*\n        wait until the operation is completed. Note this blocks until the operation is completed. \n    */\n    operationStatus.get() \n\n    //To wait up to a specified duration you can pass the max block duration\n    operationStatus.get(50, TimeUnit.MILLISECONDS) \n    /*\n        the future is blocked for utmost 50 milliseconds. \n        If the operation is not completed by then the operation is cancelled the the thread is unblocked. \n    */\n}", 
            "title": "Adding Data to EVCache"
        }, 
        {
            "location": "/using/#retrieving-data-from-evcache", 
            "text": "To retrieve the value associated with a key:  String value = myCache. String get( key );", 
            "title": "Retrieving data from EVCache"
        }, 
        {
            "location": "/using/#delete-data-from-evcache", 
            "text": "To delete the data from EVCache  Future Boolean [] status = myCache.delete( key );  Similar to set operation you can inspect the status flag to determine if the operation was successful or not.", 
            "title": "Delete data from EVCache:"
        }, 
        {
            "location": "/using/#writing-test-cases", 
            "text": "To Init EVCache when AutoBinding is on  //Make sure to turn on autobinding by setting the below property\n//platform.NFLifecycleUnitTester.ignoringAllAutoBindClasses=false\n\n//Init NFLifecycleUnitTester \nfinal NFLifecycleUnitTester unitTester = new NFLifecycleUnitTester(props);\n\n//Get instance of EVCache.Builder\nfinal EVCache.Builder evCacheBuilder = unitTester.getInjector().getInstance(EVCache.Builder.class);\n\n//Get instance of EVCache. Now you can perform all operations on it\nfinal EVCache evCache = evCacheBuilder.setAppName( EVCACHE_CINEPS ).setCacheName( cid ).enableZoneFallback().build();\n\nTo Init EVCache when AutoBinding is off\n//Make sure to turn off autobinding by setting the below property\n//platform.NFLifecycleUnitTester.ignoringAllAutoBindClasses=true\n\n//Init NFLifecycleUnitTester \nfinal NFLifecycleUnitTester unitTester = new NFLifecycleUnitTester(props, new EVCacheClientModule());\n\n//Get instance of EVCache.Builder\nfinal EVCache.Builder evCacheBuilder = unitTester.getInjector().getInstance(EVCache.Builder.class);\n\n//Get instance of EVCache. Now you can perform all operations on it\nfinal EVCache evCache = evCacheBuilder.setAppName( EVCACHE_CINEPS ).setCacheName( cid ).enableZoneFallback().build();  If you are seeing the following exception.    No implementation for com.netflix.evcache.connection.IConnectionFactoryProvider was bound.\n  while locating com.google.inject.Provider com.netflix.evcache.connection.IConnectionFactoryProvider \n    for parameter 2 at com.netflix.evcache.pool.EVCacheClientPoolManager. init (EVCacheClientPoolManager.java:86)\n  while locating com.netflix.evcache.pool.EVCacheClientPoolManager\n    for field at com.netflix.evcache.EVCache$Builder._poolManager(EVCache.java:952)\n  while locating com.netflix.evcache.EVCache$Builder\n    for parameter 0 at com.netflix.ab.client.cache.common.ABEVCache. init (ABEVCache.java:47)\n  while locating com.netflix.ab.client.cache.common.ABEVCache\n    for parameter 0 at com.netflix.ab.client.ABServiceClientManager. init (ABServiceClientManager.java:41)\n  while locating com.netflix.ab.client.ABServiceClientManager\n    for parameter 0 at com.netflix.membership.client.MembershipClientLibrary.setAbServiceClientManager(MembershipClientLibrary.java:82)\n  at com.netflix.membership.client.MembershipClientLibrary.setAbServiceClientManager(MembershipClientLibrary.java:82)\n  while locating com.netflix.membership.client.MembershipClientLibrary\n    for parameter 0 at com.netflix.msl.userauth.SwitchProfileAuthenticationFactory. init (SwitchProfileAuthenticationFactory.java:49)  You can also try to create LifecycleInjectorBuilder add EVCacheClientModule module to it. This should init all the necessary modules.   LifecycleInjectorBuilder builder = LifecycleInjector.builder();\nbuilder.withModules( new EVCacheClientModule());  You might need to set the following properties if you are not able to talk to eureka   -D@environment=test\n-D@region=us-west-2", 
            "title": "Writing Test Cases"
        }
    ]
}